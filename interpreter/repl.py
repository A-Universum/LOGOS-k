# -*- coding: utf-8 -*-
"""
–û–ù–¢–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô REPL LOGOS-Œ∫

–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è —Å–∏–º–±–∏–æ—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ-–º—ã—à–ª–µ–Ω–∏—è.
–ù–µ –ø—Ä–æ—Å—Ç–æ –∫–æ–Ω—Å–æ–ª—å ‚Äî –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –∏ —Ä–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å.
"""
import sys
import readline  # noqa: F401 ‚Äî –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å—Ç—Ä–æ–∫
from datetime import datetime
from typing import List, Dict, Any

from core.context import EnhancedActiveContext
from interpreter.lexer import OntologicalLexer
from interpreter.parser import OntologicalParser
from interpreter.evaluator import SyntheticOntologicalEvaluator


class EnhancedLOGOSREPL:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π REPL —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π:
    - –û–Ω—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∏–Ω—Ç—Ä–æ—Å–ø–µ–∫—Ü–∏–∏
    - –ñ—É—Ä–Ω–∞–ª–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    - –≠–∫—Å–ø–æ—Ä—Ç–∞ –≤ SemanticDB
    - –ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ–≥–æ –≤–≤–æ–¥–∞
    """

    def __init__(self):
        self.context = EnhancedActiveContext("repl_session")
        self.evaluator = SyntheticOntologicalEvaluator(self.context)
        self.history: List[Dict[str, Any]] = []
        self.multiline_buffer: List[str] = []

    def run(self):
        """–ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π —Å–µ—Å—Å–∏–∏."""
        print("=" * 70)
        print("üåå LOGOS-Œ∫ REPL v1.0 ‚Äî –û–Ω—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Œõ-–£–Ω–∏–≤–µ—Ä—Å—É–º–∞")
        print("üí¨ –ì–¥–µ –∫–æ–¥ ‚Äî —Ä–∏—Ç—É–∞–ª, –∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ ‚Äî –¥–∏–∞–ª–æ–≥ —Å –≠—Ñ–æ—Å–æ–º")
        print("=" * 70)
        print("–ö–æ–º–∞–Ω–¥—ã: exit, context, history, clear, analyze, save_cycle")
        print("–ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π –≤–≤–æ–¥: –Ω–∞—á–Ω–∏—Ç–µ —Å '(', –∑–∞–≤–µ—Ä—à–∏—Ç–µ —Å–∫–æ–±–∫—É ‚Äî –≤–≤–æ–¥ –∑–∞–≤–µ—Ä—à–∏—Ç—Å—è.")
        print("=" * 70)

        while True:
            try:
                prompt = "...> " if self.multiline_buffer else "ŒªŒ∫> "
                line = input(prompt).rstrip()

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ–≥–æ –≤–≤–æ–¥–∞
                if self._is_incomplete_expression(line):
                    self.multiline_buffer.append(line)
                    continue
                elif self.multiline_buffer:
                    self.multiline_buffer.append(line)
                    full_input = "\n".join(self.multiline_buffer)
                    self.multiline_buffer.clear()
                    self._process_input(full_input)
                    continue

                # –û–¥–Ω–æ—Å—Ç—Ä–æ—á–Ω—ã–π –≤–≤–æ–¥
                if not line.strip():
                    continue

                # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
                if line == "exit":
                    self._save_session_on_exit()
                    print("üëã –î–æ –≤—Å—Ç—Ä–µ—á–∏ –≤ —Å–ª–µ–¥—É—é—â–µ–º Œõ-—Ü–∏–∫–ª–µ.")
                    break
                elif line == "context":
                    self._show_context()
                    continue
                elif line == "history":
                    self._show_history()
                    continue
                elif line == "clear":
                    self._clear_context()
                    continue
                elif line == "analyze":
                    self._analyze_session()
                    continue
                elif line.startswith("save_cycle"):
                    parts = line.split()
                    operator_id = parts[1] if len(parts) > 1 else "repl_operator"
                    self._save_cycle(operator_id)
                    continue

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
                self._process_input(line)

            except KeyboardInterrupt:
                print("\n‚ùó –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'exit' –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è.")
            except EOFError:
                self._save_session_on_exit()
                print("\nüëã –°–µ—Å—Å–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
                break
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

    def _is_incomplete_expression(self, line: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –≤–≤–æ–¥ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–º S-–≤—ã—Ä–∞–∂–µ–Ω–∏–µ–º."""
        if not line.strip():
            return False
        if self.multiline_buffer:
            # –°—á–∏—Ç–∞–µ–º –±–∞–ª–∞–Ω—Å —Å–∫–æ–±–æ–∫ –≤–æ –≤—Å—ë–º –±—É—Ñ–µ—Ä–µ + —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–æ–∫–µ
            full = "\n".join(self.multiline_buffer + [line])
            return full.count('(') > full.count(')')
        else:
            return line.strip().startswith('(') and line.count('(') > line.count(')')

    def _process_input(self, source: str):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –≤–≤–µ–¥—ë–Ω–Ω—ã–π –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥."""
        lexer = OntologicalLexer(source)
        tokens = lexer.tokenize()
        phi_meta = lexer.get_phi_meta()

        parser = OntologicalParser(tokens, lexer)
        expr = parser.parse()

        if not expr:
            print("‚ÑπÔ∏è  –ü—É—Å—Ç–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ. –ù–∏—á–µ–≥–æ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ.")
            return

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ
        result = self.evaluator.eval(expr, phi_meta)
        coherence = self.context._dynamic_coherence()

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        print(f"‚áí {result}")
        if phi_meta:
            print(f"üí≠ Œ¶-–Ω–∞–º–µ—Ä–µ–Ω–∏–µ: {' | '.join(phi_meta)}")
        print(f"üìä –ö–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å: {coherence:.2%}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.history.append({
            'timestamp': datetime.now().isoformat(),
            'input': source,
            'result': str(result),
            'coherence': coherence,
            'phi_meta': phi_meta
        })

    def _show_context(self):
        """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."""
        summary = self.context.get_summary()
        print("\n" + "=" * 60)
        print("üúÇ –¢–ï–ö–£–©–ò–ô –û–ù–¢–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ö–û–ù–¢–ï–ö–°–¢")
        print("=" * 60)
        print(f"–ò–º—è —Å–µ—Å—Å–∏–∏: {summary['name']}")
        print(f"–û–ø–µ—Ä–∞—Ç–æ—Ä: {summary['operator_id'] or 'anonymous'}")
        print(f"–°—É—â–Ω–æ—Å—Ç–∏: {summary['graph_metrics']['nodes']}")
        print(f"–°–≤—è–∑–∏: {summary['graph_metrics']['edges']}")
        print(f"–ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —É–∑–ª—ã: {summary['graph_metrics']['isolated_nodes']}")
        print(f"–ö–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å: {summary['current_coherence']:.2%}")
        print(f"–¢—Ä–µ–Ω–¥: {summary['recent_activity']['coherence_trend']}")
        print(f"–ù–∞–ø—Ä—è–∂–µ–Ω–∏—è: {summary['ontological_health']['active_tensions']}")
        print(f"Œ¶-–¥–∏–∞–ª–æ–≥–æ–≤: {summary['ontological_health']['phi_dialogues']}")
        print(f"–°–ª–µ–ø—ã–µ –ø—è—Ç–Ω–∞: {list(summary['blinds_spots'].keys())}")
        print("=" * 60)

    def _show_history(self):
        """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 15 –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π."""
        print("\n" + "=" * 60)
        print("üìú –ò–°–¢–û–†–ò–Ø Œõ-–¶–ò–ö–õ–û–í (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 15)")
        print("=" * 60)
        for i, entry in enumerate(self.history[-15:], 1):
            inp = entry['input'].replace('\n', ' ')[:60]
            coh = entry['coherence']
            print(f"{i:2d}. {inp}...")
            print(f"    ‚áí {entry['result']} (–∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å: {coh:.2%})")
            if entry['phi_meta']:
                print(f"    üí≠ {', '.join(entry['phi_meta'])}")
        print("=" * 60)

    def _clear_context(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç –æ–Ω—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç."""
        name = self.context.name
        self.context = EnhancedActiveContext(name)
        self.evaluator = SyntheticOntologicalEvaluator(self.context)
        print("‚ôªÔ∏è  –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–±—Ä–æ—à–µ–Ω. –ù–æ–≤–æ–µ –æ–Ω—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ.")

    def _analyze_session(self):
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–Ω–∞—á–∏–º–æ—Å—Ç—å —Å–µ—Å—Å–∏–∏."""
        significant = [
            e for e in self.history
            if e['coherence'] < 0.5 or e['phi_meta']
        ]
        print(f"\nüîç –ê–Ω–∞–ª–∏–∑ —Å–µ—Å—Å–∏–∏:")
        print(f"  –í—Å–µ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏–π: {len(self.history)}")
        print(f"  –ó–Ω–∞—á–∏–º—ã—Ö —Å–æ–±—ã—Ç–∏–π: {len(significant)}")
        if self.context.tension_log:
            print(f"  –ê–∫—Ç–∏–≤–Ω—ã—Ö –Ω–∞–ø—Ä—è–∂–µ–Ω–∏–π: {len(self.context.tension_log)}")
        print(f"  –¢–µ–∫—É—â–∞—è –∫–æ–≥–µ—Ä–µ–Ω—Ç–Ω–æ—Å—Ç—å: {self.context._dynamic_coherence():.2%}")

    def _save_cycle(self, operator_id: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—É—â–∏–π —Ü–∏–∫–ª –≤ SemanticDB."""
        if not self.history:
            print("‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.")
            return

        cycle_data = {
            'cycle_id': f"repl_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            'timestamp': datetime.now().isoformat(),
            'expressions_evaluated': len(self.history),
            'results': [e['result'] for e in self.history],
            'final_coherence': self.context._dynamic_coherence(),
            'phi_dialogues_count': len(self.context.phi_dialogues),
            'operator_id': operator_id,
            'fair_care_enabled': self.context._fair_care_enabled
        }

        import os
        os.makedirs("semantic_db", exist_ok=True)
        path = f"semantic_db/{operator_id}_{cycle_data['cycle_id']}.yaml"
        self.evaluator.semantic_db.export_cycle(cycle_data, path)
        print(f"üíæ –¶–∏–∫–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {path}")

    def _save_session_on_exit(self):
        """–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–µ—Å—Å–∏—é –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ."""
        if self.history and input("–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–µ—Å—Å–∏—é –≤ SemanticDB? (y/N): ").lower() == 'y':
            op = input("–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 'repl_exit'): ") or "repl_exit"
            self._save_cycle(op)